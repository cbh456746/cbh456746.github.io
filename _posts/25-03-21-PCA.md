---
layout:     post
title:      "주성분 분석 (PCA: Principal Component Analysis"
subtitle:   "post_subtitle"
date:       2025-03-21 12:00:00
author:     "BH CHOI"
header-img: "img/post-bg-2015.jpg"
catalog: true
mathjax true
tags:
    - Data Science
---
### 주성분 분석 (PCA: Principal Component Analysis)

#### 1. 개요
주성분 분석(PCA)은 고차원 데이터를 저차원으로 변환하는 **차원 축소 기법**이다. 데이터의 분산을 최대한 유지하면서 중요 정보를 보존하고, 데이터 시각화 및 노이즈 제거에도 활용된다.

---

#### 2. 주성분 분석의 필요성
- 데이터의 **차원이 높을 경우 연산량 증가** 및 **과적합(Overfitting)** 발생 가능
- 고차원 데이터를 **시각적으로 표현**(2D, 3D)하기 어려움
- **데이터 내 상관관계가 높은 변수들**을 결합하여 새로운 대표 변수를 생성 가능
- 머신러닝 및 통계 분석에서 **모델의 성능 향상**과 **해석 용이성 증가**

---

#### 3. PCA의 주요 개념
1. **주성분(Principal Component, PC)**
   - 원본 데이터의 분산을 가장 잘 설명하는 새로운 축(선형 조합)
   - 첫 번째 주성분(PC1)은 가장 많은 분산을 설명하며, 이후의 주성분은 직교(orthogonal) 관계를 유지함

2. **고유값(Eigenvalue)과 고유벡터(Eigenvector)**
   - 고유값: 주성분이 설명하는 분산의 크기
   - 고유벡터: 새로운 축(주성분)의 방향

3. **공분산 행렬(Covariance Matrix)**
   - 변수들 간의 관계를 나타내며, 이를 이용해 고유값과 고유벡터를 계산함

4. **차원 축소(Dimensionality Reduction)**
   - 고유값이 큰 주성분들을 선택하여 원본 데이터의 차원을 줄이는 과정
   - 전체 분산의 일정 비율(예: 95%)을 유지하는 주성분 개수를 선택

---

#### 4. PCA 수행 과정
1. **데이터 정규화(Standardization)**
   - 평균이 0, 분산이 1이 되도록 스케일링 (Z-score 정규화)

2. **공분산 행렬(Covariance Matrix) 계산**
   - 변수 간의 상관성을 분석

3. **고유값(Eigenvalue)과 고유벡터(Eigenvector) 계산**
   - 주성분 방향을 결정

4. **주성분 선택**
   - 고유값이 큰 주성분을 선택하여 데이터 변환

5. **차원 축소된 데이터 생성**
   - 원본 데이터를 주성분 좌표계로 변환하여 차원을 축소

---

#### 5. 주성분 선택 기준
- 누적 설명 분산 비율(Cumulative Explained Variance)이 **80~95%**를 만족하는 주성분 개수 선택
- Scree Plot을 이용하여 **고유값이 급격히 감소하는 지점(엘보 포인트, Elbow Point)**까지 선택

---

#### 6. PCA의 장점과 단점
✅ **장점**
- 차원 축소를 통해 연산량 감소 및 시각화 가능
- 노이즈 제거 효과
- 변수 간 상관관계를 반영하여 새로운 축 생성

❌ **단점**
- 원본 변수의 의미가 보존되지 않음 (해석 어려움)
- 선형적 관계를 가정하므로, 비선형 데이터에는 부적합
- 정보 손실 가능성 있음

---

#### 7. PCA 실습 (Python 코드)
```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 데이터 생성
np.random.seed(42)
data = np.random.rand(100, 5)  # 100개의 샘플, 5개의 변수

# 데이터 정규화
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# PCA 수행
pca = PCA(n_components=2)  # 2개의 주성분 선택
principal_components = pca.fit_transform(data_scaled)

# 변환된 데이터 출력
print("주성분 분석 결과:")
print(pd.DataFrame(principal_components, columns=['PC1', 'PC2']))
```

---

#### 8. PCA와 다른 차원 축소 기법 비교
| 기법 | 방식 | 특징 |
|------|------------------|----------------|
| **PCA** | 선형 변환 | 데이터의 분산을 최대한 유지 |
| **LDA (Linear Discriminant Analysis)** | 선형 변환 | 클래스 간 분산을 최대화 |
| **t-SNE** | 비선형 변환 | 시각화에 적합, 해석 어려움 |
| **UMAP** | 비선형 변환 | t-SNE보다 빠르며 군집 구조 보존 |

---

### 9. 필기 시험 예상 문제
**Q. 주성분 분석(PCA)에 대한 설명으로 옳지 않은 것은?**
1. PCA는 데이터의 차원을 축소하면서 정보 손실을 최소화하는 기법이다.
2. PCA에서 첫 번째 주성분(PC1)은 가장 적은 분산을 설명하는 축이다.
3. PCA는 변수 간의 상관관계를 반영하여 새로운 변수(주성분)를 생성한다.
4. PCA 수행 전 데이터 정규화(Standardization)가 필요할 수 있다.

**정답:** 2

**해설:**
첫 번째 주성분(PC1)은 가장 많은 분산을 설명하는 방향으로 설정된다. 따라서 2번 문장은 PCA의 개념과 맞지 않다.

---

### 📌 정리
- **PCA는 데이터의 차원을 축소하면서도 최대한 정보를 유지하는 기법**이다.
- **주성분(Principal Component)은 원본 데이터의 선형 조합으로 생성되며, 첫 번째 주성분이 가장 큰 분산을 설명**한다.
- **고유값(Eigenvalue)이 큰 주성분을 선택하여 차원을 축소**하며, 차원 축소 후 데이터는 새로운 좌표계에서 표현된다.
- **머신러닝 및 데이터 분석에서 차원 축소, 노이즈 제거, 데이터 시각화 등에 활용**된다.

---
📌 *이 문서를 기반으로 PCA 개념을 이해하고, 실습을 통해 직접 적용해보자!* 🚀
